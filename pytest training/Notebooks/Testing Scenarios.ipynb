{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c63bfed-d580-402b-bbe9-59c3912d35a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ddeb0b8-2551-45e0-abee-8be02bf556b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../ETL Testing/Setup_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd4dc23-bdb9-4efa-9d44-cf11276eb848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source_details_df\n+---+-------------+------------+----------+-------------+----------+----------+\n| Id|SrcSchemaName|SrcTableName|SrcColumns|SrcColumnType| CreatedAt|updated_at|\n+---+-------------+------------+----------+-------------+----------+----------+\n|  1|       Orders| Orders_data|    Row ID|          int|08-12-2023|11-12-2023|\n|  2|       Orders| Orders_data|  Order ID|       string|08-12-2023|11-12-2023|\n+---+-------------+------------+----------+-------------+----------+----------+\nonly showing top 2 rows\n\nTarget_details_df\n+------------+---------------------------------------+------------+------------+\n|TargetdfName|FileLocation                           |SrcTableName|refdfName   |\n+------------+---------------------------------------+------------+------------+\n|orders_df   |dbfs:/FileStore/tables/Orders_data.csv |Orders_data |reference_df|\n|returns_df  |dbfs:/FileStore/tables/Returns_data.csv|Returns_data|reference_df|\n+------------+---------------------------------------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# shows the target file name, the location of the source file, the source file name and from which reference file it has benn  referenced from.\n",
    "print('Source_details_df')\n",
    "src_details_df.show(2)  \n",
    "\n",
    "print('Target_details_df')\n",
    "tgt_details_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfaaddaf-6be3-43d6-8342-697ef44ce4ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#column comparison\n",
    "\n",
    "def column_comparison(src_details_df, tgt_details_df, spark):\n",
    "    try:\n",
    "        tgt_details = tgt_details_df.select(\n",
    "            'SrcTableName',\n",
    "            'FileLocation',\n",
    "            'TargetdfName'\n",
    "            ).collect()\n",
    "        \n",
    "        for row in tgt_details:\n",
    "            src_table = row['SrcTableName']\n",
    "            tgt_path = row['FileLocation']\n",
    "            tgt_name = row['TargetdfName']\n",
    "\n",
    "            tgt_df = spark.read.format('csv') \\\n",
    "                        .option('header', True) \\\n",
    "                        .option('inferSchema', True) \\\n",
    "                        .load(tgt_path)\n",
    "\n",
    "            tgt_cols = {col.lower() for col in tgt_df.columns}\n",
    "            # tgt_cols_set = set(map(str.lower, tgt_df.columns))\n",
    "\n",
    "            src_cols = src_details_df.filter(\n",
    "                F.col('SrcTableName') == src_table\n",
    "                ).select(\n",
    "                    'SrcColumns'\n",
    "                    ).rdd.flatMap(\n",
    "                        lambda x: x\n",
    "                        ).collect()\n",
    "                            \n",
    "            src_cols = {col.lower() for col in src_cols}\n",
    "\n",
    "            missing_cols_src = tgt_cols - src_cols\n",
    "            missing_cols_tgt = src_cols - tgt_cols\n",
    "\n",
    "            if missing_cols_src:\n",
    "                print(f\"Columns are missing in the source - {src_table}. Missing columns are : {missing_cols_src}\")\n",
    "\n",
    "            if missing_cols_tgt:\n",
    "                print(f\"Columns are missing in the target - {tgt_name}. Missing columns are : {missing_cols_tgt}\")\n",
    "\n",
    "            if not missing_cols_src and not missing_cols_tgt:\n",
    "                print(f'No missing columns between Src {src_table} and tgt {tgt_name}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'An error occured: {str(e)}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8077e2f7-4ee1-4c50-816e-1c5be7bb5f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# column_comparison(src_details_df, tgt_details_df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32d529b-19eb-4443-baae-b524a3db8ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# schema comparison - data types\n",
    "def schema_comparison(src_details_df, tgt_details_df, spark):\n",
    "    try:\n",
    "        # collecting the data of tgt_details into a single list. Each row would be a record.\n",
    "        tgt_details = tgt_details_df.select(\n",
    "                    'SrcTableName',\n",
    "                    'FileLocation',\n",
    "                    'TargetdfName'\n",
    "                    ).collect()\n",
    "                \n",
    "        for row in tgt_details:\n",
    "            src_table = row['SrcTableName']\n",
    "            tgt_path = row['FileLocation']\n",
    "            tgt_name = row['TargetdfName']\n",
    "\n",
    "            tgt_df = spark.read.format('csv') \\\n",
    "                        .option('header', True) \\\n",
    "                        .option('inferSchema', True) \\\n",
    "                        .load(tgt_path)\n",
    "\n",
    "            # Filtering based on soource table name and selecting only source column names and types\n",
    "            src_schema_df = src_details_df.filter(\n",
    "                F.col('SrcTableName') == src_table\n",
    "                ).select(\n",
    "                    F.trim(F.lower('SrcColumns')),\n",
    "                    F.trim(F.lower('SrcColumnType'))\n",
    "                    )\n",
    "\n",
    "            # extracting the name and datatype of each column in the tgt data frame\n",
    "            tgt_df_schema = [(str.lower(field.name), str.lower(field.dataType.simpleString())) for field in tgt_df.schema]\n",
    "\n",
    "            #  creating a schema with these details inroder to make it easier for comparison later\n",
    "            tgt_schema_df = spark.createDataFrame(tgt_df_schema, ['TgtColumns', 'TgtDataTypes'])\n",
    "\n",
    "            tgt_schema_df = tgt_schema_df.select(\n",
    "                F.trim('TgtColumns'), \n",
    "                F.trim('TgtDataTypes')\n",
    "                )\n",
    "\n",
    "            #  creating a set of the schema values which is in the form of a tuple of col name and dtype\n",
    "            final_src_schema = set(src_schema_df.rdd.map(tuple).collect())\n",
    "            final_tgt_schema = set(tgt_schema_df.rdd.map(tuple).collect())\n",
    "\n",
    "            if final_src_schema == final_tgt_schema:\n",
    "                print(f'Schemas are a match between Src - {src_table} and Tgt - {tgt_name}')\n",
    "            else:\n",
    "                print(f'Mismatch in schemas between Src - {src_table} and Tgt - {tgt_name}')\n",
    "                #Columns in source but missing or different in target\n",
    "                src_only_mismatch = final_src_schema - final_tgt_schema\n",
    "\n",
    "                #Columns in target but missing or different in source\n",
    "                tgt_only_mismatch = final_tgt_schema - final_src_schema\n",
    "\n",
    "                if src_only_mismatch:\n",
    "                    print(f'Mismatch or missing schemas in Source - {src_table}:  ')\n",
    "                    for col, dtype in src_only_mismatch:\n",
    "                        print(f'{col}:{dtype}')\n",
    "\n",
    "                if tgt_only_mismatch:\n",
    "                    print(f'Mismatch or missing schemas in Target - {tgt_name}:')\n",
    "                    for col, dtype in tgt_only_mismatch:\n",
    "                        print(f'{col}:{dtype}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'An error occured: {str(e)}')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "598b48a1-b485-4143-bb90-d1b7081dffcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in schemas between Src - Orders_data and Tgt - orders_df\nMismatch or missing schemas in Source - Orders_data:  \nship mode:int\norder date:string\nMismatch or missing schemas in Target - orders_df:\nship mode:string\norder date:date\nMismatch in schemas between Src - Returns_data and Tgt - returns_df\nMismatch or missing schemas in Source - Returns_data:  \nmarket:int\nMismatch or missing schemas in Target - returns_df:\nmarket:string\n"
     ]
    }
   ],
   "source": [
    "schema_comparison(src_details_df, tgt_details_df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae20aa39-92e3-4fcc-a2cd-c39a516ceadc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e10f0b-ac24-49aa-8d1b-d4964018697c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56724b12-4f0c-438e-810c-471002a2b2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b532525a-cae6-471e-b00f-76ef0d546389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba7fd4b3-7f95-4a46-9ee3-a02e979e4cd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a44ccb7-edc5-4a35-9dc7-32434ab3dcf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Testing Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d55db90-7e2c-466d-a679-f160e6c71c5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+----------+-------------+----------+----------+\n| Id|SrcSchemaName|SrcTableName|SrcColumns|SrcColumnType| CreatedAt|updated_at|\n+---+-------------+------------+----------+-------------+----------+----------+\n| 25|      Returns|Returns_data|  Returned|       string|08-12-2023|11-12-2023|\n| 26|      Returns|Returns_data|  Order ID|       string|08-12-2023|11-12-2023|\n| 27|      Returns|Returns_data|    Market|       string|08-12-2023|11-12-2023|\n+---+-------------+------------+----------+-------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "src_details_df.filter(\n",
    "    F.col('SrcTableName') == 'Returns_data'\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67635b5f-d67d-4ea5-9606-0b17b4af6afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_details_df = src_details_df.withColumn(\n",
    "    'SrcColumnType',\n",
    "    F.when(\n",
    "        (F.col('SrcTableName') == 'Returns_data') & (F.col('SrcColumns') == 'Market'),\n",
    "        'int'\n",
    "        ).otherwise(F.col('SrcColumnType'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ededcfa4-3532-4776-8bc0-a3c29be26919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+----------+-------------+----------+----------+\n| Id|SrcSchemaName|SrcTableName|SrcColumns|SrcColumnType| CreatedAt|updated_at|\n+---+-------------+------------+----------+-------------+----------+----------+\n| 27|      Returns|Returns_data|    Market|          int|08-12-2023|11-12-2023|\n+---+-------------+------------+----------+-------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "src_details_df.filter(\n",
    "    #  (F.col('SrcTableName') == 'Orders_data') & (F.col('SrcColumns') == 'Ship Mode')\n",
    "    (F.col('SrcTableName') == 'Returns_data') & (F.col('SrcColumns') == 'Market')\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ee143af-c1a3-4286-8043-37c55684b912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Testing Scenarios",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
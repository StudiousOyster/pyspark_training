{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1779530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d285d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = \"C:/Program Files/Java/jdk-11\"\n",
    "\n",
    "spark = SparkSession.builder.appName('training').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22103e5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/c:/Users/chipp/OneDrive/Desktop/Training Folder/Pyspark_Training/pyspark_training/pytest training/Notebooks/data/src.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m src_df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m            \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m            \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minferSchema\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m            \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/src.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m src_df.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chipp\\OneDrive\\Desktop\\Training Folder\\Pyspark_Training\\pyspark_training\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:307\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mself\u001b[39m.options(**options)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) != \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chipp\\OneDrive\\Desktop\\Training Folder\\Pyspark_Training\\pyspark_training\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chipp\\OneDrive\\Desktop\\Training Folder\\Pyspark_Training\\pyspark_training\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/c:/Users/chipp/OneDrive/Desktop/Training Folder/Pyspark_Training/pyspark_training/pytest training/Notebooks/data/src.csv."
     ]
    }
   ],
   "source": [
    "src_df = spark.read.format(\"csv\") \\\n",
    "            .option('header', True) \\\n",
    "            .option('inferSchema', True) \\\n",
    "            .load('data/src.csv')\n",
    "            \n",
    "src_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ecc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+-----------+-----------+--------------------+-------------------+\n",
      "|CinemaCode|          CinemaName|MovieId|TicketCount|TicketPrice|       TransactionId|      TransactionDt|\n",
      "+----------+--------------------+-------+-----------+-----------+--------------------+-------------------+\n",
      "|     CSRTR|      Castro Theatre| 374471|          1|        6.9|8E506AF6-6671-45D...|2017-12-30 07:37:39|\n",
      "|     CLFTR|       Cliff Theater| 299782|          3|        5.1|1896EDD7-46D2-472...|2018-01-01 21:04:09|\n",
      "|     BLSCM|Bleecker Street C...| 332283|          2|        5.9|5EC64B9F-3AE9-44A...|2018-01-01 06:43:35|\n",
      "|     BLSCM|Bleecker Street C...| 332283|          1|        1.8|F09DB725-F57A-487...|2018-01-01 13:02:12|\n",
      "|     CSRTR|      Castro Theatre| 374471|          1|        6.4|64998ED4-5AA2-48A...|2018-01-01 08:24:34|\n",
      "|     CSRTR|      Castro Theatre| 332283|          2|        1.5|22281511-CAD1-4F3...|2017-12-30 14:04:10|\n",
      "|     CSRTR|      Castro Theatre| 412059|          3|        7.0|BFBC75F4-7389-422...|2018-01-02 21:04:33|\n",
      "|     BTHTR|       Booth Theater| 300665|          1|        7.8|26C6E9E0-027A-4BC...|2018-01-02 01:00:52|\n",
      "|     CTPTR|     Capitol Theater| 439502|          2|        6.6|A1E23107-4140-469...|2018-01-02 02:14:50|\n",
      "|     BLSCM|Bleecker Street C...| 439502|          1|       11.6|D2E7F13C-E84A-444...|2018-01-02 22:50:46|\n",
      "|     CTPTR|     Capitol Theater| 374471|          2|        3.5|D2E7F13C-E84A-444...|2018-01-01 22:33:31|\n",
      "|     CTPTR|     Capitol Theater| 413992|          3|       11.3|9C396966-57E9-410...|2018-01-01 15:58:26|\n",
      "|     CSRTR|      Castro Theatre| 460135|          3|        5.7|E51D9737-09C6-414...|2018-01-02 08:59:20|\n",
      "|     BLSCM|Bleecker Street C...| 359749|          3|        6.7|AFDA5EC9-7C11-4C2...|2018-01-01 11:09:10|\n",
      "|     BLSCM|Bleecker Street C...| 299782|          2|       10.8|38E711BF-A3AB-4FF...|2018-01-01 01:31:11|\n",
      "|     BLSCM|Bleecker Street C...| 432607|          1|        4.0|236AECFF-6C06-4FD...|2018-01-01 04:09:08|\n",
      "|     CTPTR|     Capitol Theater| 300665|          3|        2.0|5F1B78F1-4152-4F3...|2017-12-30 00:11:09|\n",
      "|     BTHTR|       Booth Theater| 432607|          2|        8.3|6D80E877-4B38-441...|2017-12-30 06:52:42|\n",
      "|     BTHTR|       Booth Theater| 413992|          3|       10.7|CDD2F0A6-8BE1-475...|2017-12-30 02:32:17|\n",
      "|     CTPTR|     Capitol Theater| 439502|          3|        5.5|57DF1D1E-308A-454...|2018-01-01 02:54:45|\n",
      "+----------+--------------------+-------+-----------+-----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tgt_df = spark.read.format(\"csv\") \\\n",
    "            .option('header', True) \\\n",
    "            .option('inferSchema', True) \\\n",
    "            .load('data/tgt.csv')\n",
    "            \n",
    "tgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695eb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
